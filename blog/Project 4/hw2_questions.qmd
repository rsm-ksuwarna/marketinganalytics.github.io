---
title: "Poisson Regression Examples"
author: "Your Name"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

Data Description

```{python}
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt

file_path = 'blueprinty.csv'
blueprinty_data = pd.read_csv(file_path)
print(blueprinty_data.head())

summary_stats = blueprinty_data.describe()
summary_stats

```
- The average age of firms is approximately 26 years, with a range from 9 to 49 years.
- The proportion of firms using Blueprinty‚Äôs software is about 32%, with the remaining 68% not using it.
- The number of patents awarded ranges from 0 to 16, with a mean of 3.68 patents per firm.

### Histograms and means of number of patents by customer status.

```{python}
# Compare histograms of number of patents by customer status
plt.figure(figsize=(8, 6))

# Plot histogram for customers and non-customers
blueprinty_data[blueprinty_data['iscustomer'] == 1]['patents'].plot(kind='hist', bins=20, alpha=0.5, color='blue', label='Customer')
blueprinty_data[blueprinty_data['iscustomer'] == 0]['patents'].plot(kind='hist', bins=20, alpha=0.5, color='red', label='Non-Customer')
plt.title('Number of Patents by Customer Status')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.legend()
plt.show()
plt.close('all')
# Plot means of patents by customer status
plt.figure(figsize=(8, 6))
mean_pats = blueprinty_data.groupby('iscustomer')['patents'].mean()
ax = mean_pats.plot(kind='bar', color=['lightblue', 'lightgreen'])
ax.set_xticks([0, 1])
ax.set_xticklabels(['Non-Customer', 'Customer'], rotation=45)
plt.title('Average Number of Patents by Customer Status')
plt.ylabel('Average Number of Patents')
plt.show()
plt.tight_layout()
plt.close('all')
```
Here are the key observations based on the histograms and summary statistics:

#### Histograms:

- The histogram for the number of patents awarded shows that both customers (firms using Blueprinty‚Äôs software) and non-customers exhibit a similar range of patent counts, but the distribution appears slightly skewed for both groups.
- There seems to be a higher frequency of firms with 0 to 5 patents, with the non-customer group slightly more concentrated around the lower patent counts.

#### Average Number of Patents:

- The average number of patents awarded to non-customers is around 3.68, whereas the average for customers is higher, suggesting that firms using the software may be awarded more patents on average.

### Comparing regions and ages by customer status

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
region_counts = blueprinty_data.groupby(['iscustomer', 'region']).size().unstack().fillna(0)

# Display summary statistics for age by customer status
age_stats_by_customer = blueprinty_data.groupby('iscustomer')['age'].describe()
print(age_stats_by_customer)

# Plot the region distribution for customers vs non-customers
plt.figure(figsize=(10, 6))
region_counts.T.plot(kind='bar', stacked=True)
plt.title('Region Distribution by Customer Status')
plt.xlabel('Region')
plt.ylabel('Number of Firms')
plt.xticks(rotation=45)
plt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])
plt.tight_layout()
plt.show()
plt.close('all')

# Plot age distributions for customers and non-customers
plt.figure(figsize=(8, 6))
blueprinty_data[blueprinty_data['iscustomer'] == 1]['age'].plot(kind='hist', bins=20, alpha=0.5, color='blue', label='Customer')
blueprinty_data[blueprinty_data['iscustomer'] == 0]['age'].plot(kind='hist', bins=20, alpha=0.5, color='red', label='Non-Customer')

plt.title('Age Distribution by Customer Status')
plt.xlabel('Age (Years)')
plt.ylabel('Frequency')
plt.legend()
plt.tight_layout()
plt.show()
plt.close('all')
```

#### Regional Distribution:

- The regional distribution of firms shows that the number of customers (firms using Blueprinty's software) and non-customers varies by region. There may be some regional concentration of Blueprinty‚Äôs customers in certain areas. This suggests that the decision to use the software might be influenced by the firm's location.

#### Age Distribution:

- The average age of firms using Blueprinty‚Äôs software is slightly higher (mean age = 26.9 years) compared to non-customers (mean age = 26.1 years).
- Both customer and non-customer firms have a similar range of ages, but customers tend to be slightly older on average. The age distribution is fairly similar, with a slight tendency for customers to be a bit older.

These differences suggest that age and region may be systematic factors influencing the decision to use Blueprinty's software. Accounting for these variables in the model would help isolate the effect of the software on the number of patents awarded.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The mathematical likelihood for _ $Y \sim \text{Poisson}(\lambda)$ is given by the following formula:

   $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

 - ùëå is the observed number of patents awarded (the outcome variable),
 - Œª is the expected number of patents awarded (which depends on the independent variables in the model, such as age and software usage),
 - ùëå! is the factorial of  ùëå.

Log Liklihood Function: 

   $log L(\lambda) = Œ£ (Yi * log(\lambda) - \lambda - log(Yi!))$

```{python}
from math import factorial

def poisson_loglikelihood(lambda_, Y):
    """
    Log-likelihood function for Poisson regression.
    
    lambda_ : float or array-like
        The expected number of events (patents awarded).
        
    Y : array-like
        Observed number of patents awarded by each firm.
    
    Returns
    -------
    log_likelihood : float
        The log-likelihood for the Poisson model.
    """
    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log([factorial(int(y)) for y in Y]))
    return log_likelihood
```

```{python}

Y = blueprinty_data['patents'].values

# Generate a range of lambda values
lambda_values = np.linspace(0.1, 10, 100)

# Calculate log-likelihood for each lambda using the observed data
log_likelihood_values = [poisson_loglikelihood(l, Y) for l in lambda_values]

# Plotting the log-likelihood
plt.figure(figsize=(8, 6))
plt.plot(lambda_values, log_likelihood_values, label='Log-Likelihood', color='blue')
plt.title('Log-Likelihood for Different Lambda Values (Observed Patents)')
plt.xlabel('Lambda')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.legend()
plt.show()
```
We can observe that the log-likelihood increases until it reaches its maximum and then starts to decrease. The peak corresponds to the ùúÜ that best fits the observed data.

#### Derivative of the Log-Likelihood
The log-likelihood function for the Poisson distribution is:
      
   $\log L(\lambda) = \sum_{i=1}^{n} \left( Y_i \log(\lambda) - \lambda - \log(Y_i!) \right)$

Taking the first derivative of this log-likelihood function with respect to ùúÜ and setting it equal to zero will help us find the maximum likelihood estimate (MLE) for ùúÜ.
Derivative:

   $\frac{d}{d\lambda} \log L(\lambda) = \sum_{i=1}^{n} \left( \frac{Y_i}{\lambda} - 1 \right)$

Setting this derivative equal to zero to find the critical point:

   $\sum_{i=1}^{n} \left( \frac{Y_i}{\lambda} - 1 \right) = 0$

Solving for ùúÜ, we get:

   $\lambda_{\text{MLE}} = \frac{\sum_{i=1}^{n} Y_i}{n} = \bar{Y}$

This shows that the MLE for ùúÜ is simply the mean of the observed number of patents ùëåÀâ, which "feels right" because the mean of a Poisson distribution is ùúÜ.

#### Finding the MLE via Optimization
```{python}
from scipy.optimize import minimize

# Define the negative log-likelihood (since we are minimizing)
def neg_poisson_loglikelihood(lambda_, Y):
    return -poisson_loglikelihood(lambda_, Y)

# Use scipy's minimize function to find the MLE
result = minimize(neg_poisson_loglikelihood, x0=1, args=(Y,), bounds=[(0.1, 10)])

# Extract the MLE
lambda_mle = result.x[0]
print(f"The MLE for lambda is: {round(lambda_mle,4)}")
```
The Maximum Likelihood Estimate (MLE) for ùúÜ, based on the data, is approximately 3.6846. This means that the best estimate for the expected number of patents awarded, according to the Poisson distribution and the data provided, is about 3.6846 patents per firm.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

#### Updated Log-Likelihood Function
We will update the log-likelihood function to take into account the covariate matrix ùëã and the parameter vector ùõΩ. The likelihood for each observation ùëñ will be:

   $f(Y|\beta, X) = \sum_{i=1}^{n} \frac{\lambda_i^{Y_i} e^{-\lambda_i}}{Y_i!}$

   Where ùúÜùëñ = exp‚Å°(ùëãùëñ‚Ä≤ùõΩ).

The log-likelihood for the Poisson regression model is:

   $\log L(\beta) = \sum_{i=1}^{n} \left( Y_i X_i' \beta - e^{X_i' \beta} - \log(Y_i!) \right)$

Here‚Äôs the updated Poisson regression likelihood function

```{python}
import numpy as np
from scipy.special import factorial
import numpy as np
import pandas as pd
import scipy as sp
from statsmodels.tools.tools import add_constant
import statsmodels.api as sm
from scipy.special import factorial, gammaln

# Define the Poisson regression log-likelihood function
def poisson_regression_loglikelihood(beta, Y, X):
    beta = np.asarray(beta, dtype=float)
    Xb = X @ beta
    Xb = np.clip(Xb, -20, 20)  # Prevent overflow
    lambda_i = np.array([math.exp(val) for val in Xb])  # Use math.exp for robustness
    return -(np.sum(-lambda_i + Y * Xb - gammaln(Y + 1)))

```

This updated function models the number of patents as a function of firm characteristics using the exponential link function, and it computes the log-likelihood for the Poisson regression model.


```{python}
import math

# Create variables
blueprinty_data["age_centered"] = blueprinty_data["age"] - blueprinty_data["age"].mean()
blueprinty_data["age_sq"] = blueprinty_data["age_centered"] ** 2
region_dummies = pd.get_dummies(blueprinty_data["region"], prefix="region", drop_first=True)

# Construct design matrix
X = pd.concat([
    pd.Series(1, index=blueprinty_data.index, name="intercept"),
    blueprinty_data[["age_centered", "age_sq", "iscustomer"]],
    region_dummies
], axis=1)

X_matrix = X.astype(float).values
Y = blueprinty_data["patents"].astype(float).values

# Initial guess for beta (starting values for the optimization)
initial_beta = np.zeros(X_matrix.shape[1])
# Use scipy's minimize function to find the MLE (we are minimizing the negative log-likelihood)
result = sp.optimize.minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X_matrix), method='BFGS')
# Extract the MLE of the coefficients
beta_hat = result.x
# Calculate the Hessian (second derivative of the log-likelihood)
hessian = result.hess_inv
# Compute the standard errors (sqrt of diagonal of the inverse Hessian)
std_errors = np.sqrt(np.diag(hessian))
# Create a DataFrame with coefficients and standard errors
coef_table = pd.DataFrame({
    'Coefficient': beta_hat,
    'Standard Error': std_errors
}, index=X.columns)
coef_table

```
#### Cross Checking using GLM function
```{python}
import statsmodels.api as sm

# Fit the Poisson regression model using statsmodels' GLM
glm_model = sm.GLM(Y, X_matrix, family=sm.families.Poisson(), link=sm.families.links.log()).fit()

# Display the summary of the GLM model
print(glm_model.summary())

``` 

The results of the Poisson regression model, shown in the table above, present the estimated coefficients and their corresponding standard errors. These results are obtained after estimating the model using Maximum Likelihood Estimation (MLE) and calculating the Hessian matrix for the standard errors.

#### Explanation of the Results:

1. Intercept:

- The intercept term has a coefficient of 1.3447, which represents the log of the expected number of patents awarded for a firm with average age, no age squared, and not using Blueprinty‚Äôs software. The standard error is 0.0383, suggesting this estimate is fairly precise.

2. Age-Centered:

- The coefficient for age_centered is -0.00797, which implies that, holding all else constant, a one-year increase in age (relative to the average firm age) slightly decreases the expected number of patents. The standard error (0.00207) indicates this effect is statistically significant.

3. Age Squared:

- The age_sq coefficient is -0.00297, suggesting a quadratic relationship with age. As age increases, the number of patents decreases, but the effect diminishes at higher ages (due to the negative coefficient on age squared). The standard error of 0.00025 is small, indicating a precise estimate.

4. IsCustomer:

- The coefficient for iscustomer is 0.20759, meaning firms using Blueprinty‚Äôs software (iscustomer = 1) are expected to receive approximately 21% more patents than non-customers, controlling for other factors like age and region. The standard error (0.0309) suggests that this result is statistically significant.

5. Regions:

- The coefficients for the region variables (relative to the base region) show regional effects on the expected number of patents. For example, region_Northeast has a coefficient of 0.02917, meaning firms in the Northeast region tend to have slightly more patents, compared to firms in the base region. The standard errors for the regional coefficients range from 0.0436 to 0.0538.

##### Conclusion:
The model suggests that Blueprinty‚Äôs software usage has a statistically significant positive effect on the number of patents awarded, as firms using the software are predicted to have more patents. Additionally, age, age squared, and region also influence patent counts, with older firms tending to have fewer patents. These results could be used to support the claim that Blueprinty‚Äôs software has a positive impact on patent success.

#### Quantifying the Effect of Blueprinty‚Äôs Software
```{python}
# Create two new datasets: X_0 (customers=0) and X_1 (customers=1)
X_0 = X.copy()
X_0['iscustomer'] = 0  # Set iscustomer to 0 for all rows (non-customers)
X_0 = X_0.astype(float).values
X_1 = X.copy()
X_1['iscustomer'] = 1  # Set iscustomer to 1 for all rows (customers)
X_1 = X_1.astype(float).values

# Predict the number of patents for both X_0 and X_1 using the fitted model
y_pred_0 = glm_model.predict(X_0)
y_pred_1 = glm_model.predict(X_1)

# Calculate the difference in predicted patents between customers and non-customers
y_diff = y_pred_1 - y_pred_0

# Compute the average difference
avg_diff = np.mean(y_diff)

# Display the result
print(f"The average effect of being a customer on the number of patents is: {round(avg_diff, 4)}")

```
The analysis of the effect of Blueprinty's software on the number of patents awarded to firms suggests a positive impact. After fitting a Poisson regression model and computing the average difference in predicted patents for firms using the software (customers) versus those not using it (non-customers), the results indicate that customers, on average, receive 0.79 more patents than non-customers.

This suggests that Blueprinty‚Äôs software is associated with an increase in the number of patents awarded, controlling for factors such as firm age, age squared, and regional location. The software appears to have a beneficial effect on patent success, supporting the marketing claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.

These findings provide valuable insight into the potential advantages of using Blueprinty‚Äôs software and its role in improving the patent outcomes for engineering firms.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::

### Load and Explore the Data

We'll start by loading the dataset and examining the first few rows to get an understanding of the variables.
```{python}
# Load the dataset
file_path = 'airbnb.csv'
airbnb_data = pd.read_csv(file_path)

# Display the first few rows of the dataset
airbnb_data.head()
```

### Exploratory Data Analysis (EDA)
We will proceed with some basic exploratory data analysis (EDA), such as summary statistics, visualizations for the distribution of key variables like price, number_of_reviews, and room_type.
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Limit extreme price values (e.g., remove prices above the 99th percentile)
price_upper_limit = np.percentile(airbnb_data['price'], 99)
filtered_price_data = airbnb_data[airbnb_data['price'] <= price_upper_limit]

# Plot price distribution with limited values
plt.figure(figsize=(10, 6))
sns.histplot(filtered_price_data['price'], kde=True, color='blue', bins=50)
plt.title('Price Distribution')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Limit number of reviews to values up to 100
filtered_reviews_data = airbnb_data[airbnb_data['number_of_reviews'] <= 100]

# Plot number of reviews distribution with a log scale on the y-axis
plt.figure(figsize=(10, 6))
sns.histplot(filtered_reviews_data['number_of_reviews'], kde=True, color='green', bins=50)
plt.title('Number of Reviews Distribution (Up to 100) with Log Y-Axis')
plt.xlabel('Number of Reviews')
plt.ylabel('Frequency (Log Scale)')
plt.yscale('log')  # Apply log scale to the y-axis
plt.show()

# Room Type Distribution by Price (Boxplot)
plt.figure(figsize=(10, 6))
sns.boxplot(data=airbnb_data, x='room_type', y='price', palette='Set2')
plt.title('Room Type Distribution by Price')
plt.xlabel('Room Type')
plt.ylabel('Price')
plt.yscale('log')  # Log scale for price to handle large differences in price values
plt.show()

# Number of Reviews by Room Type (Bar Plot)
plt.figure(figsize=(10, 6))
sns.barplot(x='room_type', y='number_of_reviews', data=airbnb_data, palette='Set2')
plt.title('Total Number of Reviews by Room Type')
plt.xlabel('Room Type')
plt.ylabel('Total Number of Reviews')
plt.show()
```
### Data Cleaning

Next, we'll check for any missing values in relevant columns such as number_of_reviews, price, bathrooms, and bedrooms. We will handle missing values appropriately (either by dropping or imputing).
```{python}
# Check for missing values in relevant columns
relevant_columns = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'room_type']
missing_values = airbnb_data[relevant_columns].isnull().sum()

# Display the missing values
print("Missing values in relevant columns:\n", missing_values)

# Drop rows with missing values in relevant columns
airbnb_data_clean = airbnb_data.dropna(subset=relevant_columns)

# Check if any missing values remain
missing_values_after_drop = airbnb_data_clean[relevant_columns].isnull().sum()
print("\nMissing values after dropping rows with missing values:\n", missing_values_after_drop)

# Display the cleaned data (first few rows)
airbnb_data_clean.head()
```

### Poisson Regression Model
```{python}
import statsmodels.api as sm
from statsmodels.formula.api import poisson

# Fit the Poisson regression model
model = poisson('number_of_reviews ~ price + bathrooms + bedrooms + room_type', data=airbnb_data_clean).fit()

# Display the model summary
model_summary = model.summary()
print(model_summary)
```
#### Interpreting Coefficients

1. Intercept: The intercept value of 2.9065 represents the log of the expected number of reviews for a listing with an average price, 0 bathrooms, 0 bedrooms, and the base room type (likely "Entire home/apt"). This acts as the baseline when all other variables are zero.

2. Private Room: The coefficient for Private Room is -0.1398, indicating that, all else equal, listings with a private room tend to have 13.98% fewer reviews compared to the reference category, which is likely Entire home/apt. This suggests that private rooms attract fewer reviews.

3. Shared Room: The coefficient for Shared Room is -0.3895, meaning that shared room listings have 38.95% fewer reviews compared to entire homes/apartments. This suggests that shared rooms are generally less popular in terms of reviews, which might reflect lower occupancy rates or fewer bookings.

4. Price: The coefficient for price is -0.0005, meaning that for every additional dollar increase in price, the expected number of reviews decreases by approximately 0.05%. This suggests a slight negative relationship between price and the number of reviews, possibly indicating that higher-priced listings receive fewer bookings and reviews.

5. Bathrooms: The coefficient for bathrooms is -0.1052, indicating that each additional bathroom is associated with 10.52% fewer reviews. This negative relationship might reflect that larger listings (with more bathrooms) could be more expensive, leading to fewer bookings and reviews overall.

6. Bedrooms: The coefficient for bedrooms is 0.1042, meaning that each additional bedroom is associated with a 10.42% increase in the expected number of reviews. This positive relationship suggests that larger listings with more bedrooms tend to attract more reviews, likely due to higher occupancy rates.


### Predicting Reviews for New Data
```{python}
# Predict the number of reviews for all listings in the cleaned dataset
predicted_reviews = model.predict(airbnb_data_clean)

# Add predicted reviews to the dataset for further analysis
airbnb_data_clean['predicted_reviews'] = predicted_reviews

# Display the first few rows with predicted values
airbnb_data_clean[['price', 'bathrooms', 'bedrooms', 'room_type', 'number_of_reviews', 'predicted_reviews']].head()

```
#### Conclusion:
The Poisson regression model provides insight into how different variables (such as price, bathrooms, bedrooms, and room_type) affect the expected number of reviews (a proxy for bookings).

Interpreting the coefficients allows us to understand which factors have the most significant impact on review counts, which could help hosts optimize their listings for better visibility and more bookings.



