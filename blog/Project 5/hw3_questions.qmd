---
title: "Multinomial Logit Model"
author: "Krithika Suwarna"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
::::


## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.


```{python}
import pandas as pd
import numpy as np

# Load data
conjoint_data = pd.read_csv("conjoint_data.csv")

# One-hot encode brand and ad with reference levels: brand=H, ad=No
X = pd.get_dummies(conjoint_data, columns=["brand", "ad"], drop_first=True)

# Define feature and metadata columns
feature_cols = ["brand_N", "brand_P", "ad_Yes", "price"]
meta_cols = ["resp", "task", "choice"]

# Prepare final DataFrame
X_features = X[feature_cols]
meta = conjoint_data[meta_cols].reset_index(drop=True)
X_full = pd.concat([meta, X_features], axis=1)

# Reshape into 3D array (n_obs, n_alts, n_features) and choice vector
n_alts = 3
n_obs = X_full.shape[0] // n_alts
n_features = len(feature_cols)

X_arr = np.zeros((n_obs, n_alts, n_features))
y = np.zeros(n_obs, dtype=int)

for i in range(n_obs):
    start = i * n_alts
    end = start + n_alts
    X_arr[i, :, :] = X_full.iloc[start:end][feature_cols].values
    y[i] = np.argmax(X_full.iloc[start:end]['choice'].values)

# Output dimensions
print("X_arr shape:", X_arr.shape)  # (1000, 3, 4)
print("y shape:", y.shape)          # (1000,)

```

## 4. Estimation via Maximum Likelihood


```{python}
from scipy.optimize import minimize
from scipy.special import logsumexp
import numpy as np

# Define the log-likelihood function for the MNL model
def mnl_log_likelihood(beta, X_arr, y):
    """
    Compute negative log-likelihood for the MNL model.

    Parameters:
    - beta: parameter vector of shape (n_features,)
    - X_arr: array of shape (n_obs, n_alts, n_features)
    - y: vector of shape (n_obs,) indicating chosen alternative

    Returns:
    - negative log-likelihood (to minimize)
    """
    n_obs, n_alts, n_features = X_arr.shape
    utilities = np.dot(X_arr, beta)  # shape: (n_obs, n_alts)
    log_prob = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_prob = log_prob[np.arange(n_obs), y]
    return -np.sum(chosen_log_prob)

```

Using `optim()` in R or `scipy.optimize()` in Python to find the MLEs for the 4 parameters ($\beta_\text{netflix}$, $\beta_\text{prime}$, $\beta_\text{ads}$, $\beta_\text{price}$), as well as their standard errors (from the Hessian). For each parameter constructed a 95% confidence interval.

```{python}
# Initial guess
beta_init = np.zeros(X_arr.shape[2])

# Optimize using BFGS
result = minimize(
    mnl_log_likelihood,
    beta_init,
    args=(X_arr, y),
    method='BFGS',
    options={'disp': True}
)

# Extract results
beta_hat = result.x
hessian_inv = result.hess_inv  # inverse Hessian from BFGS

# Standard errors and 95% confidence intervals
se = np.sqrt(np.diag(hessian_inv))
conf_int = np.vstack([beta_hat - 1.96 * se, beta_hat + 1.96 * se]).T

# Display results
for name, est, std, ci in zip(["Netflix", "Prime", "Ads", "Price"], beta_hat, se, conf_int):
    print(f"{name:>8}: {est:.3f} ¬± {1.96*std:.3f}  (95% CI: {ci[0]:.3f}, {ci[1]:.3f})")

```


## 5. Estimation via Bayesian Methods

### Metropolis-hasting MCMC sampler of the posterior distribution

```{python}
# Define log prior: N(0,5) for first 3, N(0,1) for price
def log_prior(beta):
    logp = 0
    logp += -0.5 * np.sum((beta[:3] / 5) ** 2) - 3 * np.log(np.sqrt(2 * np.pi * 25))
    logp += -0.5 * (beta[3] ** 2) - np.log(np.sqrt(2 * np.pi))
    return logp

# Define log posterior: log-likelihood + log-prior
def log_posterior(beta, X_arr, y):
    return -mnl_log_likelihood(beta, X_arr, y) + log_prior(beta)

# Metropolis-Hastings algorithm
n_iter = 11000
burn_in = 1000
beta_dim = 4
samples = np.zeros((n_iter, beta_dim))
accepts = 0

# Initialize at zeros
current_beta = np.zeros(beta_dim)
current_log_post = log_posterior(current_beta, X_arr, y)

# Proposal standard deviations
proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])

for i in range(n_iter):
    # Propose new beta
    proposal = current_beta + np.random.normal(0, proposal_sd)
    proposal_log_post = log_posterior(proposal, X_arr, y)

    # Acceptance probability
    log_accept_ratio = proposal_log_post - current_log_post
    if np.log(np.random.rand()) < log_accept_ratio:
        current_beta = proposal
        current_log_post = proposal_log_post
        accepts += 1

    samples[i] = current_beta

# Discard burn-in
posterior_samples = samples[burn_in:]

# Calculate posterior means and 95% credible intervals
posterior_mean = posterior_samples.mean(axis=0)
posterior_ci = np.percentile(posterior_samples, [2.5, 97.5], axis=0).T

posterior_mean, posterior_ci, accepts / n_iter

```

### Trace plot of the algorithm, and the histogram of the posterior distribution.

```{python}
import matplotlib.pyplot as plt

# Choose one parameter to visualize, e.g., Netflix coefficient (index 0)
param_idx = 0
param_name = "Œ≤_Netflix"
trace = posterior_samples[:, param_idx]

# Plot trace and histogram
fig, axs = plt.subplots(2, 1, figsize=(10, 6), sharex=False)

# Trace plot
axs[0].plot(trace, color='blue')
axs[0].set_title(f"Trace Plot of {param_name}")
axs[0].set_ylabel("Parameter Value")

# Histogram
axs[1].hist(trace, bins=40, density=True, color='skyblue', edgecolor='black')
axs[1].set_title(f"Posterior Distribution of {param_name}")
axs[1].set_xlabel("Parameter Value")
axs[1].set_ylabel("Density")

plt.tight_layout()
plt.show()
```

### The 4 posterior means, standard deviations, and 95% credible intervals and comparison of the results from the Maximum Likelihood approach.

```{python}
# Calculate posterior standard deviations
posterior_std = posterior_samples.std(axis=0)

# Prepare a summary table
param_names = ["Œ≤_Netflix", "Œ≤_Prime", "Œ≤_Ads", "Œ≤_Price"]
mle_estimates = beta_hat
mle_se = se
mle_ci = conf_int

import pandas as pd

summary_df = pd.DataFrame({
    "Parameter": param_names,
    "Posterior Mean": posterior_mean,
    "Posterior Std. Dev.": posterior_std,
    "Bayesian 95% CI Lower": posterior_ci[:, 0],
    "Bayesian 95% CI Upper": posterior_ci[:, 1],
    "MLE Estimate": mle_estimates,
    "MLE Std. Error": mle_se,
    "MLE 95% CI Lower": mle_ci[:, 0],
    "MLE 95% CI Upper": mle_ci[:, 1]
})
print(summary_df)
```

## 6. Discussion

### Interpreting the Parameter Estimates
Even without knowing the true part-worths, the estimated values give clear insight into consumer preferences based on revealed choices in the conjoint tasks.

Œ≤<sub>Netflix</sub> > Œ≤<sub>Prime</sub> > 0
This implies that, all else equal, consumers prefer Netflix over Prime, and Prime over Hulu (since Hulu is the reference category). The magnitudes suggest stronger preference for Netflix.

Œ≤<sub>Ads</sub> < 0
The negative sign indicates that including advertisements makes a product less attractive to consumers. This aligns with what you'd expect in practice: ad-free experiences are preferred.

Œ≤<sub>Price</sub> < 0
A negative price coefficient is economically intuitive ‚Äî it means that as price increases, the utility of a product decreases, making it less likely to be chosen. This validates the logic of price sensitivity in consumer decision-making.

What Does Œ≤<sub>Netflix</sub> > Œ≤<sub>Prime</sub> Mean?

It means that consumers derive higher utility from Netflix than from Amazon Prime, holding ad presence and price constant. So, for two identical products (same price and ad policy), one labeled ‚ÄúNetflix‚Äù and the other ‚ÄúPrime‚Äù, consumers are more likely to choose Netflix.

## Extending to a Multi-Level (Hierarchical) MNL Model

In the standard Multinomial Logit (MNL) model, we assume that all respondents share the same set of preference parameters. This is known as a fixed-effects model and implies a homogeneous population ‚Äî that every consumer values product attributes in exactly the same way. While this simplification makes the model easier to estimate, it is often unrealistic in real-world settings where individuals exhibit different tastes, priorities, and sensitivities.

To address this limitation, we can adopt a multi-level (also known as hierarchical or random-parameter) MNL model. In this framework, each individual is allowed to have their own set of preference parameters. Specifically, each person‚Äôs vector of utility weights, ùõΩùëñ , is assumed to be drawn from a population-level distribution, typically a multivariate normal distribution with mean ùúá and covariance matrix Œ£. This allows us to model both the average preferences across the population and the individual-level deviations from that average.

Simulating data under this model involves generating a unique ùõΩùëñ for each respondent from the population distribution and then using that respondent-specific vector to simulate choices. Estimating the model, in turn, requires recovering both the distributional parameters ( ùúá and 
Œ£) and, optionally, the individual-level parameters (ùõΩùëñ). This is more computationally intensive and typically requires Bayesian methods, such as Markov Chain Monte Carlo (MCMC), or advanced frequentist approaches like simulated maximum likelihood.

The hierarchical MNL model is especially useful in analyzing real-world conjoint data because it captures heterogeneity in preferences ‚Äî a critical feature when designing products, segmenting markets, or setting personalized pricing strategies. By allowing for individual-level variation, the model provides deeper and more realistic insights into consumer decision-making.













